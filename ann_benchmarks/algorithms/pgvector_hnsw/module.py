from ann_benchmarks.algorithms.base.module import BaseANN
import numpy
import concurrent.futures
from typing import Optional
import psutil
import psycopg
from psycopg_pool import ConnectionPool
from pgvector.psycopg import register_vector
from datetime import datetime, timezone, timedelta
import subprocess
import os
import sys
import shutil

LOAD_PARALLEL = False
EMBEDDINGS_PER_CHUNK = 1_000_000 # how many rows per hypertable chunk
QUERY="""with x as materialized (select id, embedding <=> %s as distance from public.items order by 2 limit 100) select id from x order by distance limit %s"""

MAX_DB_CONNECTIONS = 16
MAX_CREATE_INDEX_THREADS = 16
MAX_BATCH_QUERY_THREADS = 16
EMBEDDINGS_PER_COPY_BATCH = 5_000 # how many rows per COPY statement
START_TIME = datetime(2000, 1, 1, tzinfo=timezone.utc) # minimum time used for time column
CHUNK_TIME_STEP = timedelta(days=1) # how much to increment the time column by for each chunk
CHUNK_TIME_INTERVAL = "'1d'::interval"

assert(EMBEDDINGS_PER_COPY_BATCH <= EMBEDDINGS_PER_CHUNK)


class PGVectorHNSW(BaseANN):
    def __init__(self, metric: str, connection_str: str, m: int, ef_construction: int):
        self._metric: str = metric
        self._connection_str: str = connection_str
        self._m: int = m
        self._ef_construction: int = ef_construction
        self._ef_search: Optional[int] = None
        self._pool : ConnectionPool = None
        if metric == "angular":
            self._query: str = QUERY
        else:
            raise RuntimeError(f"unknown metric {metric}")
        print(f"query: {self._query}")

    def create_log_table(self, cur: psycopg.Cursor) -> None:
        cur.execute("create table if not exists public.log (id bigint not null generated by default as identity primary key, name text, start timestamptz not null default clock_timestamp(), stop timestamptz)")

    def log_start(self, conn: psycopg.Connection, name: str) -> int:
        with conn.cursor() as cur:
            cur.execute("insert into public.log (name) values (%s) returning id", (name, ))
            return int(cur.fetchone()[0])

    def log_stop(self, conn: psycopg.Connection, id: int) -> None:
        with conn.cursor() as cur:
            cur.execute("update public.log set stop = clock_timestamp() where id = %s", (id,))

    def start_pool(self):
        def configure(conn):
            register_vector(conn)
            if self._ef_search is not None:
                conn.execute("set hnsw.ef_search = %d" % self._ef_search)
                print("set hnsw.ef_search = %d" % self._ef_search)
            conn.execute("set work_mem = '8GB'")
            conn.execute("set max_parallel_workers_per_gather = 0")
            conn.execute("set enable_seqscan=0")
            conn.execute("set jit = 'off'")
            conn.commit()
        self._pool = ConnectionPool(self._connection_str, min_size=1, max_size=MAX_DB_CONNECTIONS, configure=configure)

    def does_table_exist(self, conn: psycopg.Connection) -> bool:
        table_count = 0
        with conn.cursor() as cur:
            cur.execute("select count(*) from pg_class where relname = 'items'")
            table_count = cur.fetchone()[0]
        return table_count > 0
        
    def create_table(self, conn: psycopg.Connection, dimensions: int) -> None:
        with conn.cursor() as cur:
            print("creating table...")
            cur.execute(f"create table public.items (id int, t timestamptz, embedding vector({dimensions}))")
            cur.execute("alter table public.items alter column embedding set storage plain")
            cur.execute(f"select create_hypertable('public.items'::regclass, 't'::name, chunk_time_interval=>{CHUNK_TIME_INTERVAL})")
            conn.commit()

    def load_table_binary(self, X: numpy.array) -> None:
        batches: list[numpy.array] = None
        if X.shape[0] < EMBEDDINGS_PER_COPY_BATCH:
            batches = [X]
        else:
            splits = [x for x in range(0, X.shape[0], EMBEDDINGS_PER_COPY_BATCH)][1:]
            batches = numpy.split(X, splits)
        print(f"copying {X.shape[0]} rows into table using {len(batches)} batches...")
        with self._pool.connection() as con:
            with con.cursor(binary=True) as cur:
                i = -1
                d = START_TIME - CHUNK_TIME_STEP
                for b, batch in enumerate(batches):
                    print(f"copying batch number {b} of {batch.shape[0]} rows into chunk {d}")
                    with cur.copy("copy public.items (id, t, embedding) from stdin (format binary)") as cpy:
                        cpy.set_types(['integer', 'timestamptz', 'vector'])
                        for v in batch:
                            i += 1
                            if i % EMBEDDINGS_PER_CHUNK == 0:
                                d = d + CHUNK_TIME_STEP
                            cpy.write_row((i, d, v))
                    con.commit()

    def load_table_serial(self, X: numpy.array) -> None:
        batches: list[numpy.array] = None
        if X.shape[0] < EMBEDDINGS_PER_COPY_BATCH:
            batches = [X]
        else:
            splits = [x for x in range(0, X.shape[0], EMBEDDINGS_PER_COPY_BATCH)][1:]
            batches = numpy.split(X, splits)
        print(f"copying {X.shape[0]} rows into table using {len(batches)} batches...")
        with self._pool.connection() as conn:
            with conn.cursor() as cur:
                i = -1
                d = START_TIME - CHUNK_TIME_STEP
                for b, batch in enumerate(batches):
                    print(f"copying batch number {b} of {batch.shape[0]} rows into chunk {d}")
                    with cur.copy("copy public.items (id, t, embedding) from stdin") as copy:
                        for v in batch:
                            i += 1
                            if i % EMBEDDINGS_PER_CHUNK == 0:
                                d = d + CHUNK_TIME_STEP
                            copy.write_row((i, d, v))
                    conn.commit()

    def load_table_parallel(self, X: numpy.array) -> None:
        print(f"total dataset: {X.shape[0]}")
        i = -1
        d = START_TIME - CHUNK_TIME_STEP
        cmd = f"""timescaledb-parallel-copy -connection '{self._connection_str}' -workers {MAX_DB_CONNECTIONS} -batch-size {EMBEDDINGS_PER_COPY_BATCH} -columns 'id,t,embedding' -schema public -table items -log-batches"""
        p = subprocess.Popen(args=cmd, stdin=subprocess.PIPE, stdout=sys.stdout, stderr=subprocess.STDOUT, text=True, shell=True, env=os.environ)
        for v in X:
            i += 1
            if i > 0 and i % EMBEDDINGS_PER_COPY_BATCH == 0:
                p.stdin.flush()
            if i % EMBEDDINGS_PER_CHUNK == 0:
                d = d + CHUNK_TIME_STEP
            v2 = '"[' + ",".join([f"{j}" for j in v]) + ']"'
            p.stdin.write(f"""{i},"{d.isoformat()}",{v2}\n""")
        p.stdin.flush()
        p.stdin.close()
        retcode = p.wait()
        if retcode != 0:
            raise subprocess.CalledProcessError(returncode=retcode, cmd=cmd)
        print("finished loading the table")

    def load_table(self, X: numpy.array) -> None:
        id = 0
        with self._pool.connection() as conn:
            id = self.log_start(conn, "loading table")
        if LOAD_PARALLEL and shutil.which("timescaledb-parallel-copy") is not None:
            self.load_table_parallel(X)
        else:
            self.load_table_binary(X)
        with self._pool.connection() as conn:
            self.log_stop(conn, id)

    def list_chunks(self, conn: psycopg.Connection) -> list[str]:
        with conn.cursor() as cur:
            cur.execute("""
                select format('%I.%I', chunk_schema, chunk_name)
                from timescaledb_information.chunks k
                where hypertable_schema = 'public'
                and hypertable_name = 'items'
                and not exists
                (
                    select 1
                    from pg_catalog.pg_indexes i
                    where k.chunk_schema = i.schemaname
                    and k.chunk_name = i.tablename
                    and i.indexname like '%_embedding_%'
                )
                order by k.range_start
                """)
            return [row[0] for row in cur]

    def index_table(self) -> None:
        with self._pool.connection() as conn:
            with conn.cursor() as cur:
                cur.execute(f"""create index on only public.items using hnsw (embedding vector_cosine_ops) 
                   with (m = {self._m}, ef_construction = {self._ef_construction})"""
                )
                conn.commit()

    def index_chunk(self, chunk: str) -> Optional[Exception]:
        try:
            with self._pool.connection() as conn:
                with conn.cursor() as cur:
                    cur.execute(f"""create index on only {chunk} using hnsw (embedding vector_cosine_ops) 
                        with (m = {self._m}, ef_construction = {self._ef_construction})"""
                    )
                    conn.commit()
        except Exception as x:
            return x
        return None

    def index_chunks(self, chunks: list[str]) -> None:
        id = 0
        with self._pool.connection() as conn:
            id = self.log_start(conn, "indexing")
        if len(chunks) == 0:
            return
        threads = min(MAX_CREATE_INDEX_THREADS, len(chunks))
        print(f"creating indexes using {threads} threads...")
        with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:
            future_to_chunk = {executor.submit(self.index_chunk, chunk): chunk for chunk in chunks}
            for future in concurrent.futures.as_completed(future_to_chunk):
                chunk = future_to_chunk[future]
                try:
                    x = future.result()
                except Exception as x2:
                    print(f"creating index on chunk {chunk} hit an exception: {x2}")
                else:
                    if x is not None:
                        print(f"creating index on chunk {chunk} hit an exception: {x}")
                    else:
                        print(f"created index on {chunk}")
        print("finished creating indexes")
        with self._pool.connection() as conn:
            self.log_stop(conn, id)

    def fit(self, X: numpy.array) -> None:
        # have to create the extensions before starting the connection pool
        with psycopg.connect(self._connection_str) as conn:
            with conn.cursor() as cur:
                cur.execute("create extension if not exists timescaledb")
                cur.execute("create extension if not exists vector")
                self.create_log_table(conn)
        self.start_pool()
        table_exists:bool = False
        with self._pool.connection() as conn:
            table_exists = self.does_table_exist(conn)
            if not table_exists:
                self.create_table(conn, int(X.shape[1]))
        if not table_exists:
            self.load_table(X)
        chunks: list[str] = None
        with self._pool.connection() as conn:
            chunks = self.list_chunks(conn)
        if len(chunks) > 0:
            self.index_chunks(chunks)
        #self.index_table()

    def set_query_arguments(self, ef_search):
        self._ef_search = ef_search
        #close and restart the pool to apply the new settings
        self._pool.close()
        self._pool = None
        self.start_pool()

    def get_memory_usage(self) -> Optional[float]:
        return psutil.Process().memory_info().rss / 1024

    def query(self, q: numpy.array, n: int) -> numpy.array:
        with self._pool.connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute(self._query, (q, n), binary=True, prepare=True)
                return numpy.array([id for id, in cursor.fetchall()])

    def batch_query(self, X: numpy.array, n: int) -> None:
        threads = min(MAX_BATCH_QUERY_THREADS, X.size)
        results = numpy.empty((X.shape[0], n), dtype=int)
        with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:
            futures = {executor.submit(self.query, q, n): i for i, q in enumerate(X)}
            for future in concurrent.futures.as_completed(futures):
                i = futures[future]
                try:
                    result = future.result()
                    results[i] = result
                except Exception as x2:
                    print(f"exception getting batch results: {x2}")
        self.results = results

    def get_batch_results(self) -> numpy.array:
        return self.results
    
    def __str__(self):
        return f"PGVectorHNSW(m={self._m}, ef_construction={self._ef_construction}, ef_search={self._ef_search})"
