import math

from ann_benchmarks.algorithms.base.module import BaseANN
import numpy
import concurrent.futures
from typing import Optional
import psutil
import psycopg
from psycopg_pool import ConnectionPool
from pgvector.psycopg import register_vector
from datetime import datetime, timezone, timedelta
from time import perf_counter

EMBEDDINGS_PER_PARTITION = 10_000_000 # how many rows per partition
QUERY = """select id from public.items order by embedding <=> %s limit %s"""

MAX_DB_CONNECTIONS = 16
MAX_CREATE_INDEX_THREADS = 16
MAX_BATCH_QUERY_THREADS = 16
EMBEDDINGS_PER_COPY_BATCH = 5_000 # how many rows per COPY statement
START_TIME = datetime(2000, 1, 1, tzinfo=timezone.utc) # minimum time used for time column
PARTITION_TIME_STEP = timedelta(days=1) # how much to increment the time column by for each chunk
PARTITION_TIME_INTERVAL = "'1d'::interval"

assert(EMBEDDINGS_PER_COPY_BATCH <= EMBEDDINGS_PER_PARTITION)

CONNECTION_SETTINGS = [
    "set work_mem = '2GB';",
    "set maintenance_work_mem = '8GB';"
    "set max_parallel_workers_per_gather = 0;",
    "set max_parallel_maintenance_workers = 0;"
    "set enable_seqscan=0;",
    "set jit = 'off';",
]


class Postgres(BaseANN):
    def __init__(self, metric: str, connection_str: str, m: int, ef_construction: int):
        self._metric: str = metric
        self._connection_str: str = connection_str
        self._m: int = m
        self._ef_construction: int = ef_construction
        self._ef_search: Optional[int] = None
        self._pool : ConnectionPool = None
        if metric == "angular":
            self._query: str = QUERY
        else:
            raise RuntimeError(f"unknown metric {metric}")
        print(f"query: {self._query}")

    def create_log_table(self, cur: psycopg.Cursor) -> None:
        cur.execute("create table if not exists public.log (id bigint not null generated by default as identity primary key, name text, start timestamptz not null default clock_timestamp(), stop timestamptz)")

    def log_start(self, conn: psycopg.Connection, name: str) -> int:
        with conn.cursor() as cur:
            cur.execute("insert into public.log (name) values (%s) returning id", (name, ))
            return int(cur.fetchone()[0])

    def log_stop(self, conn: psycopg.Connection, id: int) -> None:
        with conn.cursor() as cur:
            cur.execute("update public.log set stop = clock_timestamp() where id = %s", (id,))

    def start_pool(self):
        def configure(conn):
            register_vector(conn)
            if self._ef_search is not None:
                conn.execute("set hnsw.ef_search = %d" % self._ef_search)
                print("set hnsw.ef_search = %d" % self._ef_search)
            for setting in CONNECTION_SETTINGS:
                conn.execute(setting)
            conn.commit()
        self._pool = ConnectionPool(self._connection_str, min_size=1, max_size=MAX_DB_CONNECTIONS, configure=configure)

    def does_table_exist(self, conn: psycopg.Connection) -> bool:
        table_count = 0
        with conn.cursor() as cur:
            cur.execute("select count(*) from pg_class where relname = 'items'")
            table_count = cur.fetchone()[0]
        return table_count > 0

    def shared_buffers(self, conn: psycopg.Connection) -> bool:
        shared_buffers = 0
        with conn.cursor() as cur:
            sql_query = QUERY % ("$1", "$2")
            cur.execute(f"""
                        select 
                            shared_blks_hit + shared_blks_read
                        from pg_stat_statements
                        where queryid = (select queryid
                        from pg_stat_statements
                        where userid = (select oid from pg_authid where rolname = current_role)
                        and query like '{sql_query}'
                        );""")
            res = cur.fetchone()
            if res is not None:
                shared_buffers = res[0]
        return shared_buffers

    def create_table(self, conn: psycopg.Connection, dimensions: int, vectors: int) -> None:
        with conn.cursor() as cur:
            print("creating table...")
            cur.execute(f"create table public.items (id int, t timestamptz, embedding vector({dimensions})) partition by range (t)")
            cur.execute("alter table public.items alter column embedding set storage plain")

            bgn = START_TIME.strftime("%Y-%m-%d")
            partitions = math.ceil(vectors / EMBEDDINGS_PER_PARTITION)
            cur.execute(f"""
            do $block$
            declare
                _bgn timestamptz = '{bgn}'::timestamptz;
                _step interval = {PARTITION_TIME_INTERVAL};
                _num int = {partitions};
                _sql text;
            begin
                for _sql in
                (
                    select format($sql$create table public.items%s partition of public.items for values from (%L) to (%L) $sql$
                    , ltrim(to_char(x, '09'))
                    , _bgn + (_step * x)
                    , _bgn + (_step * x) + _step
                    )
                    from generate_series(0, _num - 1) x
                )
                loop
                    execute _sql;
                end loop;
            end;
            $block$;
            """)
            conn.commit()

    def load_table_binary(self, X: numpy.array) -> None:
        batches: list[numpy.array] = None
        if X.shape[0] < EMBEDDINGS_PER_COPY_BATCH:
            batches = [X]
        else:
            splits = [x for x in range(0, X.shape[0], EMBEDDINGS_PER_COPY_BATCH)][1:]
            batches = numpy.split(X, splits)
        num_batches = len(batches)
        print(f"copying {X.shape[0]} rows into table using {num_batches} batches...")
        with self._pool.connection() as con:
            with con.cursor(binary=True) as cur:
                i = 0
                for b, batch in enumerate(batches):
                    partition = i // EMBEDDINGS_PER_PARTITION
                    d = START_TIME + (PARTITION_TIME_STEP * partition)
                    partition = "public.items" + str(partition).zfill(2)
                    print(f"copying batch {b}/{num_batches} of {batch.shape[0]} rows into {partition}")
                    with cur.copy(f"copy {partition} (id, t, embedding) from stdin (format binary)") as cpy:
                        cpy.set_types(['integer', 'timestamptz', 'vector'])
                        for v in batch:
                            cpy.write_row((i, d, v))
                            i += 1
                    con.commit()

    def load_table(self, X: numpy.array) -> None:
        id = 0
        with self._pool.connection() as conn:
            id = self.log_start(conn, "loading table")
        self.load_table_binary(X)
        with self._pool.connection() as conn:
            self.log_stop(conn, id)

    def list_partitions(self, conn: psycopg.Connection) -> list[str]:
        with conn.cursor() as cur:
            cur.execute("""
                select format('%I.%I', n.nspname, k.relname)
                from pg_class p
                inner join pg_inherits i 
                on (p.relname = 'items' and p.relkind = 'p' and p.oid = i.inhparent)
                inner join pg_class k
                on (i.inhrelid = k.oid and k.relispartition)
                inner join pg_namespace n on (k.relnamespace = n.oid)
                where not exists
                (
                    select 1
                    from pg_catalog.pg_indexes i
                    where n.nspname = i.schemaname
                    and k.relname = i.tablename
                    and i.indexname like '%_embedding_%'
                )
                order by 1
                """)
            return [row[0] for row in cur]

    def index_partition(self, partition: str) -> Optional[Exception]:
        try:
            with self._pool.connection() as conn:
                with conn.cursor() as cur:
                    cur.execute(f"""create index on only {partition} using hnsw (embedding vector_cosine_ops) 
                        with (m = {self._m}, ef_construction = {self._ef_construction})"""
                    )
                    conn.commit()
        except Exception as x:
            return x
        return None

    def index_partitions(self, partitions: list[str]) -> None:
        id = 0
        with self._pool.connection() as conn:
            id = self.log_start(conn, "indexing")
        if len(partitions) == 0:
            return
        threads = min(MAX_CREATE_INDEX_THREADS, len(partitions))
        print(f"creating indexes using {threads} threads...")
        with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:
            future_to_partition = {executor.submit(self.index_partition, partition): partition for partition in partitions}
            for future in concurrent.futures.as_completed(future_to_partition):
                partition = future_to_partition[future]
                try:
                    x = future.result()
                except Exception as x2:
                    print(f"creating index on partition {partition} hit an exception: {x2}")
                else:
                    if x is not None:
                        print(f"creating index on partition {partition} hit an exception: {x}")
                    else:
                        print(f"created index on {partition}")
        print("finished creating indexes")
        with self._pool.connection() as conn:
            self.log_stop(conn, id)

    def fit(self, X: numpy.array) -> None:
        # have to create the extensions before starting the connection pool
        with psycopg.connect(self._connection_str) as conn:
            with conn.cursor() as cur:
                cur.execute("create extension if not exists vector")
                self.create_log_table(conn)
        self.start_pool()
        table_exists:bool = False
        with self._pool.connection() as conn:
            table_exists = self.does_table_exist(conn)
            if not table_exists:
                self.create_table(conn, int(X.shape[1]), int(X.shape[0]))
        if not table_exists:
            self.load_table(X)
        partitions: list[str] = None
        with self._pool.connection() as conn:
            partitions = self.list_partitions(conn)
        while len(partitions) > 0:
            self.index_partitions(partitions)
            with self._pool.connection() as conn:
                partitions = self.list_partitions(conn)

    def set_query_arguments(self, ef_search):
        self._ef_search = ef_search
        #close and restart the pool to apply the new settings
        self._pool.close()
        self._pool = None
        self.start_pool()

    def get_memory_usage(self) -> Optional[float]:
        return psutil.Process().memory_info().rss / 1024

    def query(self, q: numpy.array, n: int) -> tuple[numpy.array, float]:
        start = perf_counter()
        with self._pool.connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute(self._query, (q, n), binary=True, prepare=True)
                result = numpy.array([id for id, in cursor.fetchall()])
                elapsed = perf_counter() - start
                return result, elapsed

    def batch_query(self, X: numpy.array, n: int) -> None:
        threads = min(MAX_BATCH_QUERY_THREADS, X.size)

        with self._pool.connection() as conn:
            shared_buffers_start = self.shared_buffers(conn)

        results = numpy.empty((X.shape[0], n), dtype=int)
        latencies = numpy.empty(X.shape[0], dtype=float)
        with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:
            futures = {executor.submit(self.query, q, n): i for i, q in enumerate(X)}
            for future in concurrent.futures.as_completed(futures):
                i = futures[future]
                try:
                    result, latency = future.result()
                    results[i] = result
                    latencies[i] = latency
                except Exception as x2:
                    print(f"exception getting batch results: {x2}")
        self.results = results
        self.latencies = latencies

        with self._pool.connection() as conn:
            shared_buffers_end = self.shared_buffers(conn)

        self._query_shared_buffers = shared_buffers_end - shared_buffers_start

    def get_additional(self):
        return {"shared_buffers": self._query_shared_buffers}

    def get_batch_results(self) -> numpy.array:
        return self.results

    def get_batch_latencies(self) -> numpy.array:
        return self.latencies

    def __str__(self):
        return f"Postgres(m={self._m}, ef_construction={self._ef_construction}, ef_search={self._ef_search})"
